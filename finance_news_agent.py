import os
import re
import json
import calendar
from datetime import datetime, timedelta, timezone

import feedparser
import requests
import yfinance as yf
import google.generativeai as genai

RSS_LIST = [
    "https://news.google.com/rss/search?q=finance+OR+stock&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å°è‚¡+OR+ç¾è‚¡&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

CACHE_FILE = "data/news_cache.json"
OUT_FILE = "data/latest_report.json"
HISTORY_DIR = "data/history"


def clean_html(text: str) -> str:
    return re.sub(r"<.*?>", "", text or "")


def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data if isinstance(data, list) else []
        except Exception:
            return []
    return []


def save_cache(cache_list):
    os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache_list, f, ensure_ascii=False, indent=2)


def fetch_news(hours=24, limit=20):
    cache_list = load_cache()
    cache_set = set(cache_list)

    news = []
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)

    for rss in RSS_LIST:
        feed = feedparser.parse(rss)
        for e in feed.entries:
            if not hasattr(e, "published_parsed"):
                continue

            unix = calendar.timegm(e.published_parsed)
            dt = datetime.fromtimestamp(unix, tz=timezone.utc)

            if dt < cutoff:
                continue

            link = getattr(e, "link", None)
            if not link or link in cache_set:
                continue

            summary = clean_html(e.get("summary", ""))[:200]
            title = getattr(e, "title", "(no title)")

            news.append(
                {
                    "title": title,
                    "link": link,
                    "summary": summary,
                    "dt_utc": dt.isoformat(),
                }
            )

            cache_set.add(link)
            cache_list.append(link)

    save_cache(cache_list)
    news.sort(key=lambda x: x["dt_utc"], reverse=True)
    return news[:limit]


def ai_analyze(news):
    if not news:
        return "ğŸ“° ä»Šæ—¥ç„¡æ–°é‡å¤§è²¡ç¶“äº‹ä»¶"

    text = "\n".join([f"{n['title']} | {n['summary']}" for n in news])

    prompt = f"""
ä½ æ˜¯ç¸½é«”ç¶“æ¿Ÿåˆ†æå¸«èˆ‡å°è‚¡ç­–ç•¥ç ”ç©¶å“¡ã€‚
è«‹å°ä»¥ä¸‹æ–°èåšï¼š
1) é‡è¦æ€§æ’åºï¼ˆåˆ—å‡º 3-6 å‰‡æœ€é‡è¦ï¼‰
2) å¸‚å ´æƒ…ç·’ï¼ˆåé¢¨éšªåå¥½/é¢¨éšªè¶¨é¿/ä¸­æ€§ + åŸå› ï¼‰
3) å°è‚¡å½±éŸ¿ï¼ˆåˆ©å¤š/ä¸­æ€§/åˆ©ç©ºï¼›è‹¥å¯èƒ½é»åç”¢æ¥­ï¼‰
4) æŠ•è³‡è§€å¯Ÿï¼ˆ3-5 é»å¯æ“ä½œè§€å¯Ÿï¼Œé¿å…ä¿è­‰ç²åˆ©èªæ°£ï¼‰

æ–°èï¼š
{text}

è¼¸å‡ºæ ¼å¼ï¼š
ğŸŒŸè²¡ç¶“AIå¿«å ± {datetime.now().strftime("%Y-%m-%d")}

ğŸ“Šé‡å¤§äº‹ä»¶
ğŸ”¥å¸‚å ´æƒ…ç·’
ğŸ’°å°è‚¡å½±éŸ¿
ğŸ“ˆæŠ•è³‡è§€å¯Ÿ
"""

    api_key = os.environ.get("GEMINI_API_KEY", "").strip()
    if not api_key:
        # æ²’ key ä¹Ÿä¸è¦å£ï¼šè®“ç¶²ç«™èƒ½é¡¯ç¤ºå¸‚å ´å¿«ç…§/æ–°è
        return "ï¼ˆæœ¬æ©Ÿæ¸¬è©¦æ¨¡å¼ï¼šæœªè¨­å®š GEMINI_API_KEYï¼Œå› æ­¤ç•¥é AI åˆ†æï¼‰"

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.5-flash")
    r = model.generate_content(prompt)

    return r.text if hasattr(r, "text") else "AIåˆ†æå¤±æ•—"


def escape_md_v2(text: str) -> str:
    chars = r"\_*[]()~`>#+-=|{}.!"
    for c in chars:
        text = text.replace(c, "\\" + c)
    return text


def send_telegram(msg: str):
    token = os.environ.get("TELEGRAM_TOKEN", "").strip()
    chat_id = os.environ.get("TELEGRAM_CHAT_ID", "").strip()
    if not token or not chat_id:
        return  # æœ¬æ©Ÿå¯ä¸é€

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": escape_md_v2(msg),
        "parse_mode": "MarkdownV2",
        "disable_web_page_preview": True,
    }
    r = requests.post(url, json=payload, timeout=20)
    r.raise_for_status()


def _safe_float(x):
    try:
        if x is None:
            return None
        return float(x)
    except Exception:
        return None


def yf_quote_any(tickers):
    """
    ä¾åºå˜—è©¦å¤šå€‹ tickerï¼ŒæˆåŠŸå°±å›å‚³ (ticker_used, price, prev_close)
    """
    for tk in tickers:
        try:
            t = yf.Ticker(tk)
            fi = getattr(t, "fast_info", None)

            last = None
            prev = None

            if fi:
                last = _safe_float(fi.get("last_price") or fi.get("lastPrice"))
                prev = _safe_float(fi.get("previous_close") or fi.get("previousClose"))

            if last is None:
                hist = t.history(period="2d", interval="1d")
                if hist is not None and len(hist) >= 1:
                    last = _safe_float(hist["Close"].iloc[-1])
                    if len(hist) >= 2:
                        prev = _safe_float(hist["Close"].iloc[-2])

            if last is not None:
                return tk, last, prev
        except Exception:
            continue

    return None, None, None


def build_market_snapshot():
    """
    å›å‚³çµ¦ app ç”¨çš„ market dictï¼š
    key ä¸€å¾‹æ˜¯å›ºå®šä¸­æ–‡åç¨±ï¼ˆé¿å…é †åºäº‚ï¼‰
    value æ ¼å¼ï¼š{ok, ticker, price, prev_close, change, pct, asof_utc}
    """

    # âœ… å¯Œå°æŒ‡ï¼šyfinance å¯èƒ½æœƒæŠ½é¢¨ï¼Œæ‰€ä»¥åšå¤šä»£ç¢¼ fallback
    # ä½ å …æŒã€Œå¯Œå°æŒ‡ã€ï¼šå…ˆè©¦ FTX=Fï¼Œå†è©¦ FTX1!
    # éƒ½å¤±æ•—æ‰é€€å› ^TWIIï¼ˆå°è‚¡åŠ æ¬ŠæŒ‡æ•¸ï¼‰ç•¶æ•‘å‘½ï¼ˆå¯è‡ªè¡Œåˆªæ‰ï¼‰
    ftx_try = ["FTX=F", "FTX1!", "^TWII"]

    mapping = [
        ("å¯Œå°æŒ‡ï¼ˆFTXï¼‰", ftx_try),
        ("è²»åŠï¼ˆSOXï¼‰", ["^SOX"]),
        ("é“ç“ŠæœŸï¼ˆYMï¼‰", ["YM=F"]),
        ("ç´æŒ‡æœŸï¼ˆNQï¼‰", ["NQ=F"]),
        ("å°ç©é›» ADRï¼ˆTSMï¼‰", ["TSM"]),
        ("NVIDIAï¼ˆNVDAï¼‰", ["NVDA"]),
    ]

    market = {}
    now = datetime.now(timezone.utc).isoformat()

    for name, tickers in mapping:
        used, price, prev = yf_quote_any(tickers)

        if price is None:
            market[name] = {
                "ok": False,
                "ticker": used or (tickers[0] if tickers else ""),
                "price": None,
                "prev_close": None,
                "change": None,
                "pct": None,
                "asof_utc": now,
            }
            continue

        ch = (price - prev) if (prev is not None) else None
        pct = (ch / prev * 100) if (ch is not None and prev not in (None, 0)) else None

        market[name] = {
            "ok": True,
            "ticker": used or (tickers[0] if tickers else ""),
            "price": price,
            "prev_close": prev,
            "change": ch,
            "pct": pct,
            "asof_utc": now,
        }

    return market


def run_daily():
    news = fetch_news()
    report_text = ai_analyze(news)
    market = build_market_snapshot()

    payload = {
        "updated_at_utc": datetime.now(timezone.utc).isoformat(),
        "title": f"è²¡ç¶“AIå¿«å ± {datetime.now().strftime('%Y-%m-%d')}",
        "report": report_text,
        "news": news,
        "market": market,
    }

    os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)
    with open(OUT_FILE, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    os.makedirs(HISTORY_DIR, exist_ok=True)
    date_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    history_path = os.path.join(HISTORY_DIR, f"{date_str}.json")
    with open(history_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    send_telegram(report_text)


if __name__ == "__main__":
    run_daily()
