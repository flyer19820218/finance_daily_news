import os
import re
import json
import calendar
from datetime import datetime, timedelta, timezone

import feedparser
import requests
import google.generativeai as genai

RSS_LIST = [
    "https://news.google.com/rss/search?q=finance+OR+stock&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å°è‚¡+OR+ç¾Žè‚¡&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

CACHE_FILE = "data/news_cache.json"
OUT_FILE = "data/latest_report.json"
HISTORY_DIR = "data/history"


def clean_html(text: str) -> str:
    return re.sub(r"<.*?>", "", text or "")


def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data if isinstance(data, list) else []
        except Exception:
            return []
    return []


def save_cache(cache_list):
    os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache_list, f, ensure_ascii=False, indent=2)


def fetch_news(hours=24, limit=20):
    cache_list = load_cache()
    cache_set = set(cache_list)

    news = []
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)

    for rss in RSS_LIST:
        feed = feedparser.parse(rss)

        for e in feed.entries:
            if not hasattr(e, "published_parsed"):
                continue

            unix = calendar.timegm(e.published_parsed)
            dt = datetime.fromtimestamp(unix, tz=timezone.utc)
            if dt < cutoff:
                continue

            link = getattr(e, "link", None)
            if not link or link in cache_set:
                continue

            summary = clean_html(e.get("summary", ""))[:200]
            title = getattr(e, "title", "(no title)")

            news.append(
                {
                    "title": title,
                    "link": link,
                    "summary": summary,
                    "dt_utc": dt.isoformat(),
                }
            )

            cache_set.add(link)
            cache_list.append(link)

    save_cache(cache_list)
    news.sort(key=lambda x: x["dt_utc"], reverse=True)
    return news[:limit]


def fetch_market_snapshot():
    """
    ç”¨ Yahoo quote JSON API æŠ“å¿«ç…§ï¼ˆåœ¨ GitHub Actions é€šå¸¸æ¯” Streamlit Cloud ç©©ï¼‰
    """
    tickers = {
        "å°æŒ‡æœŸ": "TX=F",
        "ç´æŒ‡æœŸ": "NQ=F",
        "è²»åŠ": "^SOX",
        "é“ç“Š": "^DJI",
        "TSM": "TSM",
        "NVDA": "NVDA",
    }

    snapshot = {}
    url = "https://query1.finance.yahoo.com/v7/finance/quote"

    for name, ticker in tickers.items():
        try:
            r = requests.get(url, params={"symbols": ticker}, timeout=15)
            data = r.json()
            result = data.get("quoteResponse", {}).get("result", [])
            if not result:
                snapshot[name] = {"ticker": ticker, "ok": False}
                continue

            q = result[0]
            snapshot[name] = {
                "ticker": ticker,
                "ok": True,
                "price": q.get("regularMarketPrice"),
                "change": q.get("regularMarketChange"),
                "pct": q.get("regularMarketChangePercent"),
                "time": q.get("regularMarketTime"),
            }
        except Exception:
            snapshot[name] = {"ticker": ticker, "ok": False}

    return snapshot


def ai_analyze(news):
    if not news:
        return "ðŸ“° ä»Šæ—¥ç„¡æ–°é‡å¤§è²¡ç¶“äº‹ä»¶"

    text = "\n".join([f"{n['title']} | {n['summary']}" for n in news])

    prompt = f"""
ä½ æ˜¯ç¸½é«”ç¶“æ¿Ÿåˆ†æžå¸«èˆ‡å°è‚¡ç­–ç•¥ç ”ç©¶å“¡ã€‚
è«‹å°ä»¥ä¸‹æ–°èžåšï¼š
1) é‡è¦æ€§æŽ’åºï¼ˆåˆ—å‡º 3-6 å‰‡æœ€é‡è¦ï¼‰
2) å¸‚å ´æƒ…ç·’ï¼ˆåé¢¨éšªåå¥½/é¢¨éšªè¶¨é¿/ä¸­æ€§ + åŽŸå› ï¼‰
3) å°è‚¡å½±éŸ¿ï¼ˆåˆ©å¤š/ä¸­æ€§/åˆ©ç©ºï¼›è‹¥å¯èƒ½é»žåç”¢æ¥­ï¼‰
4) æŠ•è³‡è§€å¯Ÿï¼ˆ3-5 é»žå¯æ“ä½œè§€å¯Ÿï¼Œé¿å…ä¿è­‰ç²åˆ©èªžæ°£ï¼‰

æ–°èžï¼š
{text}

è¼¸å‡ºæ ¼å¼ï¼š
ðŸŒŸè²¡ç¶“AIå¿«å ± {datetime.now().strftime("%Y-%m-%d")}

ðŸ“Šé‡å¤§äº‹ä»¶
ðŸ”¥å¸‚å ´æƒ…ç·’
ðŸ’°å°è‚¡å½±éŸ¿
ðŸ“ˆæŠ•è³‡è§€å¯Ÿ
"""

    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        return "âš ï¸ æœªè¨­å®š GEMINI_API_KEYï¼ˆè«‹åˆ° GitHub Secrets è¨­å®šï¼‰"

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.5-flash")
    r = model.generate_content(prompt)

    return r.text if hasattr(r, "text") else "AIåˆ†æžå¤±æ•—"


def write_json(path: str, payload: dict):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def run_daily():
    news = fetch_news(hours=24, limit=20)
    report_text = ai_analyze(news)
    market = fetch_market_snapshot()

    now_utc = datetime.now(timezone.utc)
    payload = {
        "updated_at_utc": now_utc.isoformat(),
        "title": f"è²¡ç¶“AIå¿«å ± {now_utc.astimezone(timezone(timedelta(hours=8))).strftime('%Y-%m-%d')}",
        "report": report_text,
        "news": news,
        "market": market,
    }

    # latest
    write_json(OUT_FILE, payload)

    # historyï¼ˆæ¯å¤©ä¸€ä»½ï¼‰
    os.makedirs(HISTORY_DIR, exist_ok=True)
    history_name = now_utc.astimezone(timezone(timedelta(hours=8))).strftime("%Y-%m-%d") + ".json"
    write_json(os.path.join(HISTORY_DIR, history_name), payload)


if __name__ == "__main__":
    run_daily()
