import os
import re
import json
import calendar
from datetime import datetime, timedelta, timezone

import feedparser
import requests
import google.generativeai as genai

RSS_LIST = [
    "https://news.google.com/rss/search?q=finance+OR+stock&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=Âè∞ËÇ°+OR+ÁæéËÇ°&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

CACHE_FILE = "data/news_cache.json"
OUT_FILE = "data/latest_report.json"


# -------------------------------------------------
# Â∑•ÂÖ∑
# -------------------------------------------------
def clean_html(text: str) -> str:
    return re.sub(r"<.*?>", "", text or "")


def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data if isinstance(data, list) else []
        except:
            return []
    return []


def save_cache(cache_list):
    os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache_list, f, ensure_ascii=False, indent=2)


# -------------------------------------------------
# ÊäìÊñ∞ËÅû
# -------------------------------------------------
def fetch_news(hours=24, limit=20):
    cache_list = load_cache()
    cache_set = set(cache_list)

    news = []
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)

    for rss in RSS_LIST:
        feed = feedparser.parse(rss)

        for e in feed.entries:
            if not hasattr(e, "published_parsed"):
                continue

            unix = calendar.timegm(e.published_parsed)
            dt = datetime.fromtimestamp(unix, tz=timezone.utc)

            if dt < cutoff:
                continue

            link = getattr(e, "link", None)
            if not link or link in cache_set:
                continue

            summary = clean_html(e.get("summary", ""))[:200]
            title = getattr(e, "title", "(no title)")

            news.append(
                {
                    "title": title,
                    "link": link,
                    "summary": summary,
                    "dt_utc": dt.isoformat(),
                }
            )

            cache_set.add(link)
            cache_list.append(link)

    save_cache(cache_list)
    news.sort(key=lambda x: x["dt_utc"], reverse=True)
    return news[:limit]


# -------------------------------------------------
# ÊäìÂ∏ÇÂ†¥Âø´ÁÖßÔºàYahoo ÂÆòÊñπ JSON APIÔºâ
# -------------------------------------------------
def fetch_market_snapshot():
    tickers = {
        "Âè∞ÊåáÊúü": "TX=F",
        "Á¥çÊåáÊúü": "NQ=F",
        "Ë≤ªÂçä": "^SOX",
        "ÈÅìÁìä": "^DJI",
        "TSM": "TSM",
        "NVDA": "NVDA",
    }

    snapshot = {}

    for name, ticker in tickers.items():
        try:
            url = "https://query1.finance.yahoo.com/v7/finance/quote"
            r = requests.get(url, params={"symbols": ticker}, timeout=10)
            data = r.json()

            result = data.get("quoteResponse", {}).get("result", [])
            if not result:
                continue

            q = result[0]

            snapshot[name] = {
                "price": q.get("regularMarketPrice"),
                "change": q.get("regularMarketChange"),
                "pct": q.get("regularMarketChangePercent"),
            }

        except:
            continue

    return snapshot


# -------------------------------------------------
# AI ÂàÜÊûê
# -------------------------------------------------
def ai_analyze(news):
    if not news:
        return "üì∞ ‰ªäÊó•ÁÑ°Êñ∞ÈáçÂ§ßË≤°Á∂ì‰∫ã‰ª∂"

    text = "\n".join([f"{n['title']} | {n['summary']}" for n in news])

    prompt = f"""
‰Ω†ÊòØÁ∏ΩÈ´îÁ∂ìÊøüÂàÜÊûêÂ∏´ËàáÂè∞ËÇ°Á≠ñÁï•Á†îÁ©∂Âì°„ÄÇ
Ë´ãÂ∞ç‰ª•‰∏ãÊñ∞ËÅûÂÅöÔºö
1) ÈáçË¶ÅÊÄßÊéíÂ∫èÔºàÂàóÂá∫ 3-6 ÂâáÊúÄÈáçË¶ÅÔºâ
2) Â∏ÇÂ†¥ÊÉÖÁ∑íÔºàÂÅèÈ¢®Èö™ÂÅèÂ•Ω/È¢®Èö™Ë∂®ÈÅø/‰∏≠ÊÄß + ÂéüÂõ†Ôºâ
3) Âè∞ËÇ°ÂΩ±ÈüøÔºàÂà©Â§ö/‰∏≠ÊÄß/Âà©Á©∫ÔºõËã•ÂèØËÉΩÈªûÂêçÁî¢Ê•≠Ôºâ
4) ÊäïË≥áËßÄÂØüÔºà3-5 ÈªûÂèØÊìç‰ΩúËßÄÂØüÔºåÈÅøÂÖç‰øùË≠âÁç≤Âà©Ë™ûÊ∞£Ôºâ

Êñ∞ËÅûÔºö
{text}

Ëº∏Âá∫Ê†ºÂºèÔºö
üåüË≤°Á∂ìAIÂø´Â†± {datetime.now().strftime("%Y-%m-%d")}

üìäÈáçÂ§ß‰∫ã‰ª∂
üî•Â∏ÇÂ†¥ÊÉÖÁ∑í
üí∞Âè∞ËÇ°ÂΩ±Èüø
üìàÊäïË≥áËßÄÂØü
"""

    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        return "‚ö†Ô∏è Êú™Ë®≠ÂÆö GEMINI_API_KEY"

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.5-flash")

    r = model.generate_content(prompt)
    return r.text if hasattr(r, "text") else "AIÂàÜÊûêÂ§±Êïó"


# -------------------------------------------------
# ‰∏ªÊµÅÁ®ã
# -------------------------------------------------
def run_daily():
    news = fetch_news()
    report_text = ai_analyze(news)
    market_snapshot = fetch_market_snapshot()

    payload = {
        "updated_at_utc": datetime.now(timezone.utc).isoformat(),
        "title": f"Ë≤°Á∂ìAIÂø´Â†± {datetime.now().strftime('%Y-%m-%d')}",
        "report": report_text,
        "news": news,
        "market": market_snapshot,
    }

    os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)

    with open(OUT_FILE, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    run_daily()
