import os
import re
import json
import calendar
from datetime import datetime, timedelta, timezone
import feedparser
import requests
import google.generativeai as genai

# ================= CONFIG =================
RSS_LIST = [
    "https://news.google.com/rss/search?q=finance+OR+stock&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=Âè∞ËÇ°+OR+ÁæéËÇ°&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

CACHE_FILE = "data/news_cache.json"
OUT_FILE = "data/latest_report.json"

# ================= UTILS =================
def apply_streamlit_patch():
    """Ë¶ñË¶∫Ë¶èÁØÑË£ú‰∏ÅÔºöËß£Ê±∫ Apple iOS Ë®≠ÂÇôÂèçÈªëÂïèÈ°å"""
    try:
        import streamlit as st
        st.markdown(
            """
            <style>
            html, body, [data-testid="stAppViewContainer"] {
                background-color: #FFFFFF !important;
                color: #000000 !important;
                font-family: 'HanziPen SC', sans-serif !important;
                color-scheme: light !important;
            }
            </style>
            """,
            unsafe_allow_html=True
        )
    except ImportError:
        pass  # Êñº GitHub Actions Á¥îÂæåÁ´ØÂü∑Ë°åÊôÇËá™ÂãïÁï•ÈÅé

def clean_html(text: str) -> str:
    return re.sub(r"<.*?>", "", text or "")

def escape_md_v2(text: str) -> str:
    chars = r"\_*[]()~`>#+-=|{}.!"
    for c in chars:
        text = text.replace(c, "\\" + c)
    return text

def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data if isinstance(data, list) else []
        except (json.JSONDecodeError, IOError):
            return []
    return []

def save_cache(cache_list):
    os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache_list[-200:], f, ensure_ascii=False)

# ================= CORE LOGIC =================
def fetch_news(hours=24, limit=20):
    cache_list = load_cache()
    cache_set = set(cache_list)
    news = []
    cutoff = datetime.now(timezone.utc) - timedelta(hours=hours)

    for rss in RSS_LIST:
        feed = feedparser.parse(rss)
        for e in feed.entries:
            if not hasattr(e, "published_parsed"):
                continue

            unix = calendar.timegm(e.published_parsed)
            dt = datetime.fromtimestamp(unix, tz=timezone.utc)
            if dt < cutoff:
                continue

            link = getattr(e, "link", None)
            if not link or link in cache_set:
                continue

            summary = clean_html(e.get("summary", ""))[:200]
            title = getattr(e, "title", "(no title)")

            news.append({
                "title": title,
                "link": link,
                "summary": summary,
                "dt_utc": dt.isoformat(),
            })
            cache_set.add(link)
            cache_list.append(link)

    save_cache(cache_list)
    news.sort(key=lambda x: x["dt_utc"], reverse=True)
    return news[:limit]

def ai_analyze(news):
    if not news:
        return "üì∞ ‰ªäÊó•ÁÑ°Êñ∞ÈáçÂ§ßË≤°Á∂ì‰∫ã‰ª∂"

    text = "\n".join([f"{n['title']} | {n['summary']}" for n in news])
    prompt = f"""‰Ω†ÊòØÁ∏ΩÈ´îÁ∂ìÊøüÂàÜÊûêÂ∏´ËàáÂè∞ËÇ°Á≠ñÁï•Á†îÁ©∂Âì°„ÄÇ
Ë´ãÂ∞ç‰ª•‰∏ãÊñ∞ËÅûÂÅöÔºö
1) ÈáçË¶ÅÊÄßÊéíÂ∫èÔºàÂàóÂá∫ 3-6 ÂâáÊúÄÈáçË¶ÅÔºâ
2) Â∏ÇÂ†¥ÊÉÖÁ∑íÔºàÂÅèÈ¢®Èö™ÂÅèÂ•Ω/È¢®Èö™Ë∂®ÈÅø/‰∏≠ÊÄß + ÂéüÂõ†Ôºâ
3) Âè∞ËÇ°ÂΩ±ÈüøÔºàÂà©Â§ö/‰∏≠ÊÄß/Âà©Á©∫ÔºõËã•ÂèØËÉΩÈªûÂêçÁî¢Ê•≠Ôºâ
4) ÊäïË≥áËßÄÂØüÔºà3-5 ÈªûÂèØÊìç‰ΩúËßÄÂØüÔºåÈÅøÂÖç‰øùË≠âÁç≤Âà©Ë™ûÊ∞£Ôºâ

Êñ∞ËÅûÔºö
{text}

Ëº∏Âá∫Ê†ºÂºèÔºö
üåüË≤°Á∂ìAIÂø´Â†± {datetime.now().strftime("%Y-%m-%d")}

üìäÈáçÂ§ß‰∫ã‰ª∂
üî•Â∏ÇÂ†¥ÊÉÖÁ∑í
üí∞Âè∞ËÇ°ÂΩ±Èüø
üìàÊäïË≥áËßÄÂØü
"""

    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        return "‚ùå ÈåØË™§ÔºöÊú™Ë®≠ÂÆö GEMINI_API_KEY Áí∞Â¢ÉËÆäÊï∏"

    genai.configure(api_key=api_key)
    # ÈéñÂÆö 2.5 ÁâàÊú¨
    model = genai.GenerativeModel("gemini-2.5-flash")
    
    try:
        # Ë®≠ÂÆö 60 Áßí timeout Èò≤ÂëÜÔºåÈÅøÂÖç GitHub Actions Âç°Ê≠ª
        r = model.generate_content(prompt, request_options={"timeout": 60})
        return r.text if r.text else "AI ÂàÜÊûêÂõûÂÇ≥Á©∫ÂÄº"
    except Exception as e:
        return f"‚ùå AI ËôïÁêÜÂ§±ÊïóÔºåÈÅ≠ÈÅá API Áï∞Â∏∏: {str(e)}"

def send_telegram(msg: str):
    token = os.environ.get("TELEGRAM_TOKEN")
    chat_id = os.environ.get("TELEGRAM_CHAT_ID")
    if not token or not chat_id:
        print("‚ö†Ô∏è Ë≠¶ÂëäÔºöÁº∫Â∞ë Telegram Token Êàñ Chat IDÔºåÁï•ÈÅéÁôºÈÄÅ„ÄÇ")
        return

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {
        "chat_id": chat_id,
        "text": escape_md_v2(msg),
        "parse_mode": "MarkdownV2",
        "disable_web_page_preview": True,
    }
    r = requests.post(url, json=payload, timeout=20)
    if r.status_code != 200:
        print(f"Telegram API Error: {r.text}")

def run_daily():
    os.makedirs("data", exist_ok=True)
    news = fetch_news()
    report_text = ai_analyze(news)

    payload = {
        "updated_at_utc": datetime.now(timezone.utc).isoformat(),
        "title": f"Ë≤°Á∂ìAIÂø´Â†± {datetime.now().strftime('%Y-%m-%d')}",
        "report": report_text,
        "news": news,
    }

    with open(OUT_FILE, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    send_telegram(report_text)
    print("‚úÖ ‰ªªÂãôÂü∑Ë°åÂÆåÁï¢„ÄÇ")

if __name__ == "__main__":
    apply_streamlit_patch()
    run_daily()
