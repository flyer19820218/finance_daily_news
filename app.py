import os
import json
from datetime import datetime, timedelta, timezone, date

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup

import yfinance as yf

LATEST_FILE = "data/latest_report.json"
HISTORY_DIR = "data/history"

st.set_page_config(page_title="è²¡ç¶“AIå¿«å ±", page_icon="ğŸ“ˆ", layout="wide")

st.title("ğŸ“ˆ è²¡ç¶“AIå¿«å ±")
st.caption("æ¯æ—¥ 06:00ï¼ˆå°åŒ—ï¼‰è‡ªå‹•æ›´æ–°ï½œTelegram æ¨æ’­åŒæ­¥ï½œé‡å¤§äº‹ä»¶æ’åºï½œå°è‚¡å½±éŸ¿åˆ¤è®€ï½œæŠ•è³‡è§€å¯Ÿ")

# -------------------------
# Data helpers (report)
# -------------------------
@st.cache_data(ttl=60)
def load_json(path: str):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def list_history_files():
    if not os.path.exists(HISTORY_DIR):
        return []
    files = [fn for fn in os.listdir(HISTORY_DIR) if fn.endswith(".json")]
    files.sort(reverse=True)
    return files

def fmt_updated(updated_at_utc: str) -> str:
    try:
        dt_utc = datetime.fromisoformat(updated_at_utc.replace("Z", "")).replace(tzinfo=timezone.utc)
        return dt_utc.strftime("%Y-%m-%d %H:%M UTC")
    except Exception:
        return updated_at_utc or "-"

# -------------------------
# Market data helpers
# -------------------------
@st.cache_data(ttl=60 * 30)  # 30 mins cache
def yf_series(ticker: str, period: str = "6mo", interval: str = "1d") -> pd.Series:
    df = yf.download(ticker, period=period, interval=interval, progress=False, auto_adjust=True)
    if df is None or df.empty:
        return pd.Series(dtype=float)
    s = df["Close"].dropna()
    s.name = ticker
    return s

@st.cache_data(ttl=60 * 60)  # 1 hour cache
def taifex_txf_close(days: int = 90) -> pd.Series:
    """
    å˜—è©¦æŠ“æœŸäº¤æ‰€(taifex) TX(å°æŒ‡æœŸ) è¿‘ N å¤©æ”¶ç›¤/çµç®—è³‡æ–™ã€‚
    ç”±æ–¼æœŸäº¤æ‰€é é¢æ ¼å¼å¯èƒ½è®Šå‹•ï¼Œè‹¥æŠ“ä¸åˆ°æœƒå›å‚³ç©º Seriesã€‚
    """
    # æœŸäº¤æ‰€æ—¥å ±é€šå¸¸éœ€è¦é€æ—¥æŸ¥ï¼›é€™è£¡åšã€Œè¿‘å¹¾å¤©ã€é€æ—¥æŠ“å–ï¼ˆæœ‰ cacheï¼Œé¿å…é‡è¤‡æ‰“ï¼‰
    end = date.today()
    start = end - timedelta(days=days)

    rows = []
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0"
    })

    # é€æ—¥æŠ“ï¼ˆåªå–æœ‰è³‡æ–™çš„äº¤æ˜“æ—¥ï¼‰
    d = start
    while d <= end:
        # é¿å…å¤ªé »ç¹ï¼šåœ¨ Streamlit Cloud å…¶å¯¦é‚„å¥½ï¼Œä¸”æœ‰ cache
        query_date = d.strftime("%Y/%m/%d")

        # æœŸäº¤æ‰€ã€ŒæœŸè²¨æ¯æ—¥è¡Œæƒ…ã€(HTML)
        # commodity_id=TX æ˜¯å°æŒ‡æœŸ (TX)ï¼›ä¸åŒç«™é»å¯èƒ½åƒæ•¸åç•¥ä¸åŒ
        url = "https://www.taifex.com.tw/cht/3/futDailyMarketReport"
        params = {
            "queryType": "2",
            "marketCode": "0",
            "commodity_id": "TX",
            "queryDate": query_date,
            "MarketCode": "0",
            "commodityId": "TX",
        }

        try:
            r = session.get(url, params=params, timeout=15)
            if r.status_code != 200 or not r.text:
                d += timedelta(days=1)
                continue

            # ç”¨ pandas read_html æŠ“è¡¨æ ¼æ¯”è¼ƒç©©ï¼ˆlxml å·²åœ¨ requirementsï¼‰
            tables = pd.read_html(r.text)
            if not tables:
                d += timedelta(days=1)
                continue

            # é€šå¸¸ç¬¬ä¸€å€‹å¤§è¡¨å°±æ˜¯è¡Œæƒ…è¡¨ï¼›æˆ‘å€‘æ‰¾ã€Œæ”¶ç›¤åƒ¹ã€æˆ–ã€Œçµç®—åƒ¹ã€æ¬„ä½
            found = None
            for t in tables:
                cols = [str(c) for c in t.columns]
                if any("æ”¶ç›¤" in c for c in cols) or any("çµç®—" in c for c in cols):
                    found = t
                    break

            if found is None or found.empty:
                d += timedelta(days=1)
                continue

            # æœ‰äº›è¡¨æœƒåŒ…å«å¤šå€‹åˆ°æœŸæœˆä»½ï¼›å–ç¬¬ä¸€åˆ—ï¼ˆè¿‘æœˆï¼‰æˆ–ã€Œåˆ°æœŸæœˆä»½ã€æœ€å°è€…
            # æ¬„ä½åç¨±å¯èƒ½æ˜¯ï¼šåˆ°æœŸæœˆä»½(é€±åˆ¥)ã€æ”¶ç›¤åƒ¹ã€çµç®—åƒ¹
            t = found.copy()

            # å˜—è©¦æ‰¾ã€Œåˆ°æœŸæœˆä»½ã€æ¬„
            month_col = None
            for c in t.columns:
                if "åˆ°æœŸ" in str(c) and ("æœˆ" in str(c) or "é€±" in str(c)):
                    month_col = c
                    break

            if month_col is not None:
                # å–æ’åºå¾Œç¬¬ä¸€ç­†ï¼ˆè¿‘æœˆï¼‰
                t = t.sort_values(by=month_col, ascending=True)

            # æ”¶ç›¤åƒ¹å„ªå…ˆï¼Œæ²’æœ‰å°±ç”¨çµç®—åƒ¹
            close_col = None
            for c in t.columns:
                if "æ”¶ç›¤" in str(c):
                    close_col = c
                    break
            if close_col is None:
                for c in t.columns:
                    if "çµç®—" in str(c):
                        close_col = c
                        break

            if close_col is None:
                d += timedelta(days=1)
                continue

            val = t.iloc[0][close_col]
            try:
                val = float(str(val).replace(",", "").strip())
            except Exception:
                d += timedelta(days=1)
                continue

            rows.append((pd.to_datetime(d), val))

        except Exception:
            # ç•¶å¤©æŠ“ä¸åˆ°å°±è·³é
            pass

        d += timedelta(days=1)

    if not rows:
        return pd.Series(dtype=float)

    s = pd.Series({dt: v for dt, v in rows}).sort_index()
    s.name = "TAIFEX:TXF (TX)"
    return s

def plot_series(title: str, s: pd.Series):
    if s is None or s.empty:
        st.warning(f"{title}ï¼šè³‡æ–™æŠ“ä¸åˆ°ï¼ˆå¯èƒ½è³‡æ–™æºé™åˆ¶æˆ–ç•¶å‰ç¶²è·¯/æ ¼å¼è®Šå‹•ï¼‰")
        return
    df = s.to_frame("Close")
    st.write(f"**{title}**")
    fig = plt.figure()
    plt.plot(df.index, df["Close"])
    plt.xticks(rotation=0)
    st.pyplot(fig, clear_figure=True)

# -------------------------
# TOP charts
# -------------------------
st.subheader("ğŸŒ å…¨çƒé‡è¦è‚¡å¸‚ / æœŸè²¨ / å€‹è‚¡ï¼ˆè‡ªå·±æŠ“è³‡æ–™ç¹ªåœ–ï¼‰")
mobile = st.toggle("ğŸ“± æ‰‹æ©Ÿæ¨¡å¼ï¼ˆçª„è¢å¹•ç”¨ï¼‰", value=False)

# 1) å°æŒ‡æœŸï¼šå…ˆæŠ“ TAIFEXï¼ŒæŠ“ä¸åˆ° fallback ^TWIIï¼ˆåŠ æ¬Šï¼‰
txf = taifex_txf_close(days=120)
if txf.empty:
    txf = yf_series("^TWII", period="6mo")  # fallback
    txf_title = "å°æŒ‡æœŸï¼ˆæŠ“ä¸åˆ° TXF æ™‚ä»¥åŠ æ¬Š ^TWII ä»£æ›¿ï¼‰"
else:
    txf_title = "å°æŒ‡æœŸï¼ˆTAIFEX TX è¿‘æœˆï¼‰"

# å…¶é¤˜ï¼šyfinance
sox = yf_series("^SOX", period="6mo")
ymf = yf_series("YM=F", period="6mo")   # é“ç“ŠæœŸ
ndx = yf_series("^NDX", period="6mo")   # ç´æŒ‡ï¼ˆä¹Ÿå¯æ”¹ NQ=Fï¼‰
tsm = yf_series("TSM", period="6mo")
nvda = yf_series("NVDA", period="6mo")

series_list = [
    (txf_title, txf),
    ("è²»åŠï¼ˆ^SOXï¼‰", sox),
    ("é“ç“ŠæœŸï¼ˆYM=Fï¼‰", ymf),
    ("ç´æŒ‡ï¼ˆ^NDXï¼‰", ndx),
    ("å°ç©é›» ADRï¼ˆTSMï¼‰", tsm),
    ("NVIDIAï¼ˆNVDAï¼‰", nvda),
]

if mobile:
    for title, s in series_list:
        plot_series(title, s)
else:
    # å…©åˆ—ä¸‰æ¬„
    r1 = series_list[:3]
    r2 = series_list[3:]

    c1, c2, c3 = st.columns(3)
    with c1: plot_series(*r1[0])
    with c2: plot_series(*r1[1])
    with c3: plot_series(*r1[2])

    c4, c5, c6 = st.columns(3)
    with c4: plot_series(*r2[0])
    with c5: plot_series(*r2[1])
    with c6: plot_series(*r2[2])

st.divider()

# -------------------------
# Report section
# -------------------------
st.subheader("ğŸ“° å¿«å ±å…§å®¹")
history_files = list_history_files()
mode = st.radio("é¡¯ç¤ºå…§å®¹", ["æœ€æ–°ï¼ˆä»Šæ—¥ï¼‰", "æ­·å²å›é¡§"], horizontal=True)

data = None
label = "ä»Šæ—¥"

if mode == "æœ€æ–°ï¼ˆä»Šæ—¥ï¼‰":
    data = load_json(LATEST_FILE)
else:
    if not history_files:
        st.warning("ç›®å‰æ²’æœ‰æ­·å²å ±å‘Šï¼ˆè«‹ç¢ºèª agent æœ‰å­˜ data/history/YYYY-MM-DD.json ä¸” workflow æœ‰ git add data/historyï¼‰ã€‚")
        st.stop()
    pick = st.selectbox("é¸æ“‡æ—¥æœŸ", history_files, index=0)
    data = load_json(os.path.join(HISTORY_DIR, pick))
    label = pick.replace(".json", "")

if not data:
    st.warning("å°šæœªç”¢ç”Ÿå ±å‘Šï¼ˆæˆ–æª”æ¡ˆè®€å–å¤±æ•—ï¼‰ã€‚è«‹ç¢ºèª GitHub Actions æ˜¯å¦æˆåŠŸåŸ·è¡Œã€‚")
    st.stop()

st.info(f"é¡¯ç¤ºï¼š{label}ï½œæœ€å¾Œæ›´æ–°ï¼š{fmt_updated(data.get('updated_at_utc',''))}")

if mobile:
    st.subheader("ğŸ§  AI å¿«å ±")
    st.markdown(data.get("report", ""))

    st.subheader("ğŸ—ï¸ æ–°èåˆ—è¡¨")
    q = st.text_input("æœå°‹ï¼ˆæ¨™é¡Œ/æ‘˜è¦ï¼‰", placeholder="ä¾‹å¦‚ï¼šFedã€CPIã€å°ç©é›»ã€AIã€æ²¹åƒ¹â€¦")
    news = data.get("news", [])
    if q:
        ql = q.lower()
        news = [n for n in news if ql in (n.get("title","") + " " + n.get("summary","")).lower()]
    st.write(f"å…± {len(news)} å‰‡")
    for n in news:
        with st.container(border=True):
            st.markdown(f"**{n.get('title','')}**")
            if n.get("link"):
                st.markdown(f"[é–±è®€åŸæ–‡]({n.get('link')})")
            with st.expander("æ‘˜è¦"):
                st.write(n.get("summary",""))
else:
    left, right = st.columns([1.35, 0.65], gap="large")
    with left:
        st.subheader("ğŸ§  AI å¿«å ±")
        st.markdown(data.get("report", ""))
    with right:
        st.subheader("ğŸ—ï¸ æ–°èåˆ—è¡¨")
        q = st.text_input("æœå°‹ï¼ˆæ¨™é¡Œ/æ‘˜è¦ï¼‰", placeholder="ä¾‹å¦‚ï¼šFedã€CPIã€å°ç©é›»ã€AIã€æ²¹åƒ¹â€¦")
        news = data.get("news", [])
        if q:
            ql = q.lower()
            news = [n for n in news if ql in (n.get("title","") + " " + n.get("summary","")).lower()]
        st.write(f"å…± {len(news)} å‰‡")
        for n in news:
            with st.container(border=True):
                st.markdown(f"**{n.get('title','')}**")
                if n.get("link"):
                    st.markdown(f"[é–±è®€åŸæ–‡]({n.get('link')})")
                with st.expander("æ‘˜è¦"):
                    st.write(n.get("summary",""))
